'''

# 4.2 Token Statistics - heaps()

heaps() analyzes the processed tokens and counts the number of unique tokens.

We do not count stopwords if they are being removed, but do count the extra words that hyphens and other non-period punctuations generate. The return should have the format: (10, 8), (20, 17), ... , where the first component is the number of tokens generated so far, and the second component is the number of those tokens so far if you remove duplicates.

For both counts, these are the tokens after the process of tokenizing, stopping, and stemming steps (indicated by the options in tokenization()), not before those steps. Clearly, the second count will never be greater than the first.

To reduce the number of tuples generated by 90%, we skip any pair where the first number is not a multiple of 10, except for the final pair, which should always be outputted (total number of tokens, total number of unique tokens) even if 10 does not divide it.

'''

def heaps(processed_tokens: list[tuple[str, list[str]]]) -> list[tuple[int, int]]:
    """
    Analyze the processed tokens.

    Args:
        processed_tokens: the list of processed tokens in the expected format.

    Returns: an array of statistics in required format, i.e.,
        [
            (10, 8),
            (20, 17),
            ...
        ].
    """


    if len(processed_tokens) == 0:
      return [(0,0)]

    seen = set()
    x = 0
    y = 0
    toops = []
    for phrase, tokens in processed_tokens:
      for tok in tokens:
        if tok not in seen:
          seen.add(tok)
          y = y + 1
        x = x + 1
        if x % 10 == 0:
          toops.append((x, y))
    if x % 10 != 0:
      toops.append((x,y))

    return toops

processed_tokens = tokenization(train_sentences, stopwords=stopwords, tokenizer_type="fancy", stemming_type="porter")
heaps_result = heaps(processed_tokens)
[print(curr) for curr in heaps_result[:10]];




'''



# 4.3 Token Statistics - statistics()

statistics() analyze the statistic of the processed tokens and return the following information in a tuple:

  <ul>
      <li>the first element is the total number of tokens you encountered</li>
      <li>the second is the total number of unique tokens you encountered</li>
      <li>the third element is an array contains the 100 most frequent final tokens you generated &ndash; i.e., after all of tokenizing, stopping, and stemming are done. Each element should have the format <code>[token, token_count]</code> where <code>token_count</code> is the number of occurrences of <code>token</code> you found. Sort them in descending order by the value of <code>token_count</code>. If there are multiple words with the same value of <code>token_count</code>, sort them alphabetically by <code>token</code>. If the same value of <code>token_num</code> happens for the 100th and 101st items, just stop at 100 with a tie breaker based on alphabetical order, for example, between (apple, 100) and (banana, 100), the result should be (apple, 100).</li>
  </ul>

Note that the first total number of tokens and unique tokens should match with the last element of the result of heaps().


'''


import heapq

def statistics(
    processed_tokens: list[tuple[str, list[str]]], top_k: int = 100
) -> tuple[int, int, list[tuple[str, int]]]:
    """
    Analyze the processed tokens.

    Args:
        processed_tokens: the list of processed tokens in the expected format.

    Returns:
        total number of tokens encountered;
        total number of unique tokens encountered;
        100 most frequent processed tokens encountered in required format, i.e.,
        [
            ("Test", 10),
            ("Token", 10),
            ("Apple", 9),
            ...
        ].
    """

    if len(processed_tokens) == 0:
      return [0, 0, ('', 0)]

    seen = {}
    x = 0
    y = 0
    for phrase, tokens in processed_tokens:
      for tok in tokens:
        if tok not in seen:
          seen[tok] = 1
          y = y + 1
        else:
          seen[tok] = seen[tok] + 1
        x = x + 1

    toops = [(k, v) for k, v in seen.items()]
    toops = sorted(toops, key = lambda z: (-z[1], z[0]))
    toops = toops[0:top_k]
    ret = (x, y, toops)

    return ret

processed_tokens = tokenization(train_sentences, stopwords=stopwords, tokenizer_type="fancy", stemming_type="porter")
stats_result = statistics(processed_tokens, top_k=100)
print(f"Overall Token Counts: {stats_result[0]}; Overall Unique Token Counts: {stats_result[1]}")
print("Top 10 frequent tokens with count:")
[print(curr) for curr in stats_result[2][:10]];


processed_toks = tokenization(sentences, stopwords=stopwords, tokenizer_type="fancy", stemming_type="porter")
stats_res = statistics(processed_toks, top_k=100)
print("Top 100 frequent words with count:")
[print(w) for w in stats_result[2][:100]];




'''

# 5.3 Graph Comparison between result and Heaps' law - graph_comparison()

'''


import matplotlib.pyplot as plt
import numpy as np


def graph_comparison(heaps_result: list[tuple[int, int]], K: float, B: float) -> None:
    """
    Generates a graph that compares the results of processed documents and that of heaps' law.
    The curvature of heaps' law is computed using the function f(x) = K*x^B.

    Args:
        heaps_result: output from the heaps() function
        K: heaps' law parameter,
        B: heaps' law parameter.
    """

    heaps = np.array(heaps_result)

    X = heaps[:,0]
    dataset_y = heaps[:,1]
    heaps_y = [K * (elem ** B) for elem in X]

    _, ax = plt.subplots(1)
    ax.plot(X, heaps_y, "--", label=f"Heaps {K} {B}")
    ax.plot(X, dataset_y, label="Dataset")
    ax.set_title("Vocabulary growth for the Dataset")
    ax.set_xlabel("Words in Collection")
    ax.set_ylabel("Words in Vocabulary")
    ax.set_xlim(xmin=0)
    ax.set_ylim(ymin=0)
    plt.legend()
    plt.show()



heaps_stuff = heaps(tokenization(train_sentences, stopwords=stopwords, tokenizer_type="fancy", stemming_type="porter"))

K = 2.35
B = 0.79
graph_comparison(heaps_result=heaps_stuff, K=K, B=B)

heaps_stuff = heaps(tokenization(sentences, stopwords=stopwords, tokenizer_type="fancy", stemming_type="porter"))
K = 24
B = 0.483
graph_comparison(heaps_result=heaps_stuff, K=K, B=B)


'''

# 6 - Zipf curve

'''



def zipf_graph_comparison(stats_info: tuple[int, int, list[tuple[str, int]]], c: float = None) -> None:
    """
    Generates a graph that compares the results of processed documents and that of zipf's law.
    The curvature of zipf's law is computed using the function f(x) = c/x, where x is the rank.

    Args:
        stats_info: the outputs from the statistics() function
        c: zipf's law parameter. Using rank 1's probability if not specified.
    """

    total_words = stats_info[0]
    freqs = np.array((stats_info[2]))[:,1].astype(int)
    X = [i for i in range(1, len(freqs) + 1)]
    if c is None:
      c = freqs[0]/total_words
    dataset_y = [freq/total_words for freq in freqs]
    zips_y = [c/elem for elem in X]

    _, ax = plt.subplots(1)
    ax.plot(X, zips_y, "--", label=f"Zips {c}")
    ax.plot(X, dataset_y, label="Dataset")
    ax.set_title("Probability of Occurence for the Dataset")
    ax.set_xlabel("Rank of Words (decreasing frequency)")
    ax.set_ylabel("Probability of Occurence")
    ax.set_xlim(xmin=0)
    ax.set_ylim(ymin=0, ymax=0.05)
    plt.legend()
    plt.show()


stats_numbers = statistics(tokenization(train_sentences, stopwords=stopwords, tokenizer_type="fancy", stemming_type="porter"))

zipf_graph_comparison(stats_numbers, c=.085)




















